# 强化学习简介

强化学习 (RL, Reinforcement Learning)，是基本的机器学习范式之一（仅次于监督学习 (Supervised Learning) 和无监督学习(Unsupervised Learning)）。强化学习和「策略」息息相关：它应当产生正确的策略，或从错误的策略中学习。

假设有一个模拟环境，比如说股市。当我们用某一个规则来限制这个市场时，会发生什么？这个规则（或者说策略）有积极或消极的影响吗？如果它的影响是正面的，我们需要从这种_负面强化_中学习，改变我们的策略。如果它的影响是正面的，我们需要在这种_积极强化_的基础上再进一步发展。

![彼得和狼](../images/peter.png)

> 彼得和他的朋友们得从饥饿的狼这儿逃掉！图片来自 [Jen Looper](https://twitter.com/jenlooper)

## 本节主题：彼得与狼（俄罗斯）

[彼得与狼](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) 是俄罗斯作曲家[谢尔盖·普罗科菲耶夫](https://en.wikipedia.org/wiki/Sergei_Prokofiev)创作的音乐童话。它讲述了彼得勇敢地走出家门，到森林中央追逐狼的故事。在本节中，我们将训练帮助 Peter 追狼的机器学习算法：

- **探索**周边区域并构建最佳地图
- **学习**如何使用滑板并在滑板上保持平衡，以便更快地移动。

[![彼得和狼](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 🎥 点击上面的图片，听普罗科菲耶夫的《彼得与狼》

## 强化学习

在前面的部分中，您已经看到了两类机器学习问题的例子：

- **监督**，在有已经标记的，暗含解决方案的数据集的情况下。 [分类](../../4-Classification/README.md) 和 [回归](../../2-Regression/README.md) 是监督学习任务。
- **无监督**，在我们没有标记训练数据集的情况下。无监督学习的主要例子是 [聚类](../../5-Clustering/README.md)。

在本节中，我们将学习一类新的机器学习问题，它不需要已经标记的训练数据 —— 比如这两类问题：

- **[半监督学习](https://wikipedia.org/wiki/Semi-supervised_learning)**，在我们有很多未标记的、可以用来预训练模型的数据的情况下。
- **[强化学习](https://wikipedia.org/wiki/Reinforcement_learning)**，在这种方法中，机器通过在某种模拟环境中进行实验来学习最佳策略。

### 示例 - 电脑游戏

假设我们要教会计算机玩某一款游戏 —— 例如国际象棋，或者 [超级马里奥](https://wikipedia.org/wiki/Super_Mario)。为了让计算机学会玩游戏，我们需要它预测在每个游戏「状态」下，它应该做什么「操作」。虽然这看起来像是一个分类问题，但事实并非如此，因为我们并没有像这样的，包含「状态」和状态对应的「操作」的数据集。我们只有一些有限的数据，比如来自国际象棋比赛的记录，或者是玩家玩超级马里奥的记录。这些数据可能无法涵盖足够多的「状态」。

不同于这种需要大量现有的数据的方法，**强化学习**是基于*让计算机多次玩*并观察玩的结果的想法。因此，要使用强化学习方法，我们需要两个要素：

- **环境**和**模拟器**，它们允许我们多次玩游戏。该模拟器应该定义所有游戏规则，以及可能的状态和动作。

- **奖励函数**，它会告诉我们每个每一步（或者每局游戏）的表现如何。

其他类型的机器学习和强化学习 (RL) 之间的主要区别在于，在 RL 中，我们通常在完成游戏之前，都不知道我们是赢还是输。因此，我们不能说单独的某个动作是不是「好」的 - 我们只会在游戏结束时获得奖励。我们的目标是设计算法，使我们能够在这种不确定的条件下训练模型。我们将了解一种称为 **Q-learning** 的 RL 算法。

## 课程

1.【强化学习和 Q-Learning 介绍】(1-QLearning/README.md)
2.【使用 Gym 模拟环境】(2-Gym/README.md)

## 本文作者

“强化学习简介” 由 [Dmitry Soshnikov](http://soshnikov.com) 用 ♥️ 编写